# ğŸ§­ Filled Ballot Validation â€” Revised Implementation Plan
### Version 2.0 â€” Surgical Integration Approach

**Date:** 2025-10-29  
**Status:** Implementation Ready  
**Approach:** Leverage existing infrastructure, minimal new code

---

## ğŸ¯ Objective

Validate that **bubble detection (OMR appreciation)** remains accurate when ballots are:
- Rotated (Â±3Â° to Â±10Â°)
- Skewed (shear 2Â° to 6Â°)
- Perspective-distorted (ratio 0.90-1.00)

**Key Difference from Original Plan:**
- âœ… Reuse existing test infrastructure
- âœ… Leverage PHP-generated filled ballots
- âœ… Integrate as scenario-6 (not separate system)
- âœ… Build on quality gate work (Phase 1-3)

---

## ğŸ“Š What We Already Have

### âœ… Existing Infrastructure:
1. **Distortion generator**: `scripts/synthesize_ballot_variants.py`
2. **Quality metrics**: `packages/omr-appreciation/omr-python/quality_metrics.py`
3. **Fiducial detection**: ArUco marker detection working
4. **Test framework**: `scripts/test-omr-appreciation.sh` with 5 scenarios
5. **Filled ballots**: Generated by `OMRAppreciationTest.php` (scenario-1)

### âŒ What's Missing:
1. Distorted filled ballot fixtures
2. Ground truth JSON for validation
3. Result comparison script
4. Scenario-6 integration

---

## ğŸ”§ Implementation Steps (Surgical Approach)

### Step 1: Extract Filled Ballot Base (5 min)

**Action:** Use existing scenario-1 filled ballot as source

```bash
# Get latest test run
LATEST=$(readlink storage/app/tests/omr-appreciation/latest)

# Copy filled ballot as base template
cp storage/app/tests/omr-appreciation/${LATEST}/scenario-1-normal/filled.png \
   storage/app/tests/omr-appreciation/fixtures/filled-ballot-base.png
```

**Output:** `fixtures/filled-ballot-base.png` (baseline filled ballot)

---

### Step 2: Generate Distorted Filled Ballots (10 min)

**Action:** Apply distortions to filled ballot

```bash
# Generate 9 distorted variants
python3 scripts/synthesize_ballot_variants.py \
  --input storage/app/tests/omr-appreciation/fixtures/filled-ballot-base.png \
  --output storage/app/tests/omr-appreciation/fixtures/filled-distorted
```

**Output:** 
- `filled-distorted/U0_reference_upright.png`
- `filled-distorted/R1_rotation_+3deg.png`
- `filled-distorted/R2_rotation_+10deg.png`
- ... (9 variants total)

---

### Step 3: Create Ground Truth JSON (5 min)

**Action:** Document expected bubble states

**File:** `storage/app/tests/omr-appreciation/fixtures/filled-ballot-ground-truth.json`

```json
{
  "test_id": "filled_ballot_validation",
  "description": "Expected bubble states for scenario-1 normal ballot",
  "expected_marks": {
    "PRESIDENT": ["LD_001"],
    "VICE-PRESIDENT": ["VD_002"],
    "SENATOR": ["JD_001", "ES_002", "MF_003"]
  },
  "bubble_states": {
    "PRESIDENT_LD_001": true,
    "VICE-PRESIDENT_VD_002": true,
    "SENATOR_JD_001": true,
    "SENATOR_ES_002": true,
    "SENATOR_MF_003": true
  },
  "total_expected_marks": 5,
  "total_bubbles": 50
}
```

**Source:** Extract from `OMRAppreciationTest.php` test expectations

---

### Step 4: Create Comparison Script (30 min)

**Action:** Build validation script

**File:** `scripts/compare_appreciation_results.py`

```python
#!/usr/bin/env python3
"""
Compare appreciation results against ground truth.

Usage:
    python3 compare_appreciation_results.py \
        --result results.json \
        --truth ground-truth.json \
        --output report.json
"""
import json
import sys
import argparse
from typing import Dict, Tuple

def compare_results(result: Dict, truth: Dict) -> Dict:
    """Compare appreciation results against ground truth."""
    
    expected = truth['bubble_states']
    detected = result.get('marks', {})
    
    tp = fp = fn = tn = 0
    errors = []
    
    for bubble_id, expected_state in expected.items():
        detected_state = bubble_id in detected
        
        if expected_state and detected_state:
            tp += 1
        elif expected_state and not detected_state:
            fn += 1
            errors.append({
                'bubble': bubble_id,
                'error': 'false_negative',
                'expected': True,
                'detected': False
            })
        elif not expected_state and detected_state:
            fp += 1
            errors.append({
                'bubble': bubble_id,
                'error': 'false_positive',
                'expected': False,
                'detected': True
            })
        else:
            tn += 1
    
    total = tp + fp + fn + tn
    accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'true_positives': tp,
        'false_positives': fp,
        'false_negatives': fn,
        'true_negatives': tn,
        'total': total,
        'errors': errors,
        'verdict': 'PASS' if accuracy >= 0.98 else 'FAIL'
    }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--result', required=True)
    parser.add_argument('--truth', required=True)
    parser.add_argument('--output', required=True)
    args = parser.parse_args()
    
    with open(args.result) as f:
        result = json.load(f)
    with open(args.truth) as f:
        truth = json.load(f)
    
    comparison = compare_results(result, truth)
    
    with open(args.output, 'w') as f:
        json.dump(comparison, f, indent=2)
    
    # Print summary
    print(f"Accuracy: {comparison['accuracy']*100:.2f}%")
    print(f"Precision: {comparison['precision']*100:.2f}%")
    print(f"Recall: {comparison['recall']*100:.2f}%")
    print(f"Verdict: {comparison['verdict']}")
    
    sys.exit(0 if comparison['verdict'] == 'PASS' else 1)

if __name__ == '__main__':
    main()
```

---

### Step 5: Integrate as Scenario-6 (40 min)

**Action:** Add scenario-6-filled-ballot-validation to test script

**File:** `scripts/test-omr-appreciation.sh`

**Location:** After scenario-5-quality-gates (line ~200)

```bash
# Run scenario-6-filled-ballot-validation if fixtures exist
FILLED_FIXTURE_DIR="storage/app/tests/omr-appreciation/fixtures/filled-distorted"
FILLED_GROUND_TRUTH="storage/app/tests/omr-appreciation/fixtures/filled-ballot-ground-truth.json"

if [ -d "${FILLED_FIXTURE_DIR}" ] && [ -f "${FILLED_GROUND_TRUTH}" ]; then
    SCENARIO_6="${RUN_DIR}/scenario-6-filled-ballot-validation"
    mkdir -p "${SCENARIO_6}"
    
    echo -e "${YELLOW}Testing filled ballot validation...${NC}"
    
    # Test each distorted filled ballot
    VALIDATION_PASSED=0
    VALIDATION_FAILED=0
    
    for fixture in "${FILLED_FIXTURE_DIR}"/*.png; do
        [ -f "${fixture}" ] || continue
        basename=$(basename "${fixture}" .png)
        echo -e "  Validating ${BLUE}${basename}${NC}..."
        
        # Run appreciation (placeholder - integrate with actual appreciation)
        # TODO: Call appreciation engine on fixture
        # appreciation_result="${SCENARIO_6}/${basename}_appreciation.json"
        
        # Compare results
        # python3 scripts/compare_appreciation_results.py \
        #     --result "${appreciation_result}" \
        #     --truth "${FILLED_GROUND_TRUTH}" \
        #     --output "${SCENARIO_6}/${basename}_validation.json"
        
        # For now, mark as pending
        echo -e "    ${YELLOW}â³ PENDING (appreciation integration)${NC}"
    done
    
    echo -e "${GREEN}âœ“ Filled ballot validation complete${NC}"
    echo ""
fi
```

---

## ğŸ“Š Success Metrics

| Metric | Threshold | Priority |
|--------|-----------|----------|
| **Mark Accuracy** | â‰¥98% | ğŸ”´ Critical |
| **False Positive Rate** | â‰¤1% | ğŸŸ¡ Important |
| **False Negative Rate** | â‰¤2% | ğŸŸ¡ Important |
| **Upright vs 3Â° rotation** | Î” accuracy â‰¤2% | ğŸŸ¢ Nice-to-have |

---

## ğŸ¯ Expected Results

| Fixture | Distortion | Expected Accuracy | Verdict |
|---------|------------|-------------------|---------|
| U0 | None | 100% | âœ… PASS |
| R1 | +3Â° rotation | â‰¥98% | âœ… PASS |
| R2 | +10Â° rotation | â‰¥95% | âš ï¸ WARN |
| S1 | 2Â° shear | â‰¥98% | âœ… PASS |
| S2 | 6Â° shear | â‰¥95% | âš ï¸ WARN |
| P1-P3 | Perspective | â‰¥98% | âœ… PASS |

---

## ğŸš€ Implementation Order

1. âœ… Extract filled ballot base (Step 1) â€” 5 min
2. âœ… Generate distorted fixtures (Step 2) â€” 10 min
3. âœ… Create ground truth JSON (Step 3) â€” 5 min
4. âœ… Build comparison script (Step 4) â€” 30 min
5. â¸ï¸ Integrate scenario-6 (Step 5) â€” 40 min

**Total:** ~1.5 hours (surgical, no duplicate work)

---

## ğŸ“ Notes

### Why This Approach is Better:

1. **Reuses infrastructure** â€” No duplicate generators or test frameworks
2. **Surgical integration** â€” Extends existing scenarios cleanly
3. **Faster** â€” 1.5 hours vs 4+ hours for full rewrite
4. **Maintainable** â€” Uses established patterns
5. **Testable** â€” Can validate incrementally

### Limitations:

- Requires appreciation engine integration (not in scope for this phase)
- Ground truth must match exact bubble IDs from template
- PHP test fixtures may differ from production ballots

### Future Enhancements:

- Add confidence score tracking
- Compare confidence deltas across distortions
- Overlay visualization (green/red circles on bubbles)
- Historical accuracy trending

---

## âœ… Done When:

- [ ] Filled ballot base extracted
- [ ] 9 distorted variants generated
- [ ] Ground truth JSON created
- [ ] Comparison script validated
- [ ] Scenario-6 integrated and documented
- [ ] Test run produces validation report

---

**Status:** Ready for implementation  
**Estimated Time:** 1.5 hours  
**Dependencies:** Existing test infrastructure (complete)  
**Blocker:** None
